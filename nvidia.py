# -*- coding: utf-8 -*-
"""NVIDIA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SU3VbJgZvtJngQ_zindHk3Slvvuhl1gH
"""

import pandas as pd
import re
import numpy as np
import requests
d = pd.read_csv('nasdaq_exteral_data.csv')

filtered = d[d['Stock_symbol'].notnull()]
filtered = filtered[filtered['Url'].notnull()]
filtered = filtered[filtered['Article'].notnull()]

filtered = filtered[filtered['Stock_symbol'] == "NVDA"]
for x in filtered["Url"]:
  print(x)

seed = 903987527
sampled_articles = filtered.sample(n=15, random_state=np.random.seed(seed))

for url in sampled_articles[['Url']].iloc:
  print(url)

!pip install nbconvert
!jupyter nbconvert --to html '/content/AS3 (1).ipynb'

import requests
from bs4 import BeautifulSoup
import nltk
import re

# Download NLTK tokenizer data if not already present
nltk.download('punkt', quiet=True)
from nltk.tokenize import sent_tokenize

def scrape_article(url):
    """
    Scrapes the full text from a Nasdaq article URL.
    Adjust the BeautifulSoup search criteria if the HTML structure changes.
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; ArticleScraper/1.0)"
    }
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"Error fetching page: {url}")
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # Attempt to locate the main article text.
    # This may need to be updated if Nasdaq changes their site structure.
    article_container = soup.find('div', class_=re.compile(r'(article-body|content|article-main)'))
    if article_container:
        # Join text segments while preserving some spacing.
        text = article_container.get_text(separator=' ')
    else:
        # Fallback: extract all text from the page.
        text = soup.get_text(separator=' ')

    # Clean up the text by removing extra whitespace.
    cleaned_text = " ".join(text.split())
    return cleaned_text

def split_into_sentences(text):
    """
    Splits the article text into individual sentences using NLTK's sentence tokenizer.
    """
    sentences = sent_tokenize(text)
    return sentences

def process_articles(urls):
    """
    Processes each URL: scrapes the article and splits it into sentences.
    """
    articles_data = {}
    for url in urls:
        print(f"Processing: {url}")
        article_text = scrape_article(url)
        if article_text:
            sentences = split_into_sentences(article_text)
            articles_data[url] = sentences
            print(f"Found {len(sentences)} sentences.")
        else:
            articles_data[url] = []
    return articles_data

if __name__ == '__main__':
    # List of Nasdaq article URLs
    urls = [
        "https://www.nasdaq.com/articles/1-artificial-intelligence-ai-growth-stock-to-buy-hand-over-fist-and-1-that-i-wouldnt-touch",
        "https://www.nasdaq.com/articles/nvidias-auto-software-opportunity-is-underappreciated-says-wells-fargo",
        "https://www.nasdaq.com/articles/the-q3-earnings-season-kicks-off-positively-0",
        "https://www.nasdaq.com/articles/wait-until-nvidia-stock-hits-%24220...-then-buy",
        "https://www.nasdaq.com/articles/advanced-energy-aeis-expands-portfolio-with-artesyn-csu3200et",
        "https://www.nasdaq.com/articles/the-zacks-analyst-blog-highlights-nvidia-synopsys-meta-broadcom-and-microsoft",
        "https://www.nasdaq.com/articles/nvidia-stock-little-changed-on-earnings-miss-revenue-beat",
        "https://www.nasdaq.com/articles/3-seriously-undervalued-semiconductor-stocks-to-buy-now",
        "https://www.nasdaq.com/articles/electric-vehicles-artificial-intelligence-is-it-time-to-buy-teslas-stock",
        "https://www.nasdaq.com/articles/tuesday-sector-leaders%3A-technology-communications-services"
    ]

    articles = process_articles(urls)